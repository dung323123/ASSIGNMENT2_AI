from typing import Optional, List
import numpy as np
import torch
import torch.nn as nn
from src.game.board import GameState, Move, Player, RED, BLACK
from src.train_ml import ValueNet

class MLAgent:

    def __init__(
        self, 
        player_symbol: Player, 
        model_path: str = "value_net.pt", 
        device: Optional[str] = None,
        exploration_rate: float = 0.05  # 5% kh√°m ph√° ng·∫´u nhi√™n
    ):
        self.player = player_symbol
        self.exploration_rate = exploration_rate
        self.device = torch.device(device or ("cuda" if torch.cuda.is_available() else "cpu"))
        
        # Load ML model
        self.model = ValueNet(input_dim=91).to(self.device)
        state_dict = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(state_dict)
        self.model.eval()
        print(f"[MLAgent] ü§ñ Loaded Value Network (NO search)")
        print(f"[MLAgent] üé≤ Exploration rate: {exploration_rate:.1%}")
        
        # Ch·ªëng l·∫∑p ƒë∆°n gi·∫£n
        self.position_history = []
        self.max_history = 20
        
        # Stats
        self.moves_evaluated = 0
    
    # ============================================================
    # ENCODING
    # ============================================================
    def encode_state(self, state: GameState) -> np.ndarray:
        """Encode state th√†nh 91 features"""
        board_flat = state.board.flatten().astype(np.float32)
        cur_player = np.array([float(state.current_player)], np.float32)
        return np.concatenate([board_flat, cur_player], axis=0)
    
    # ============================================================
    # EVALUATION: Pure ML (No Heuristic)
    # ============================================================
    def evaluate_state(self, state: GameState) -> float:
        """
        ƒê√°nh gi√° state b·∫±ng Value Network
        Tr·∫£ v·ªÅ ƒëi·ªÉm t·ª´ g√≥c nh√¨n self.player
        """
        # Game over
        if state.is_game_over():
            if state.current_player == self.player:
                return -1000.0  # M√¨nh thua
            else:
                return 1000.0   # ƒê·ªãch thua
        
        # ML prediction
        features = self.encode_state(state)
        x = torch.tensor(features, dtype=torch.float32, device=self.device).unsqueeze(0)
        
        with torch.no_grad():
            value = self.model(x).view(-1).item()
        
        # value ‚àà [-1, 1], chuy·ªÉn v·ªÅ g√≥c nh√¨n self.player
        if state.current_player == self.player:
            return value
        else:
            return -value
    
    # ============================================================
    # REPETITION CHECK
    # ============================================================
    def is_repetition(self, state: GameState) -> bool:
        """Ki·ªÉm tra l·∫∑p v·ªã tr√≠"""
        key = tuple(state.board.flatten())
        return self.position_history.count(key) >= 2
    
    # ============================================================
    # GET MOVE: Greedy 1-ply (No Search)
    # ============================================================
    def get_move(self, state: GameState) -> Optional[Move]:
        """
        Ch·ªçn n∆∞·ªõc ƒëi t·ªët nh·∫•t (greedy):
        1. ƒê√°nh gi√° t·∫•t c·∫£ n∆∞·ªõc ƒëi h·ª£p l·ªá
        2. Ch·ªçn n∆∞·ªõc c√≥ eval cao nh·∫•t
        3. Tr√°nh l·∫∑p
        """
        moves = state.get_all_legal_moves()
        if not moves:
            return None
        
        self.moves_evaluated = len(moves)
        
        # Exploration: 5% kh√°m ph√° ng·∫´u nhi√™n (gi√∫p tr√°nh local minima)
        if np.random.random() < self.exploration_rate:
            move = moves[np.random.randint(len(moves))]
            print(f"[MLAgent] üé≤ Random exploration")
            self._update_history(state)
            return move
        
        # ƒê√°nh gi√° t·ª´ng n∆∞·ªõc ƒëi
        best_score = -1e9
        best_move = None
        
        for move in moves:
            next_state = state.make_move(move)
            
            # Ph·∫°t l·∫∑p (nh·∫π h∆°n so v·ªõi search-based agent)
            if self.is_repetition(next_state):
                score = -10.0  # Ph·∫°t nh·∫π
            else:
                # ML evaluation
                score = self.evaluate_state(next_state)
            
            if score > best_score:
                best_score = score
                best_move = move
        
        print(f"[MLAgent] ‚úì Evaluated {len(moves)} moves, best score: {best_score:.3f}")
        
        # C·∫≠p nh·∫≠t history
        self._update_history(state)
        
        return best_move
    
    def _update_history(self, state: GameState):
        """C·∫≠p nh·∫≠t l·ªãch s·ª≠ v·ªã tr√≠"""
        key = tuple(state.board.flatten())
        self.position_history.append(key)
        if len(self.position_history) > self.max_history:
            self.position_history.pop(0)
    
    # ============================================================
    # UTILITIES
    # ============================================================
    def reset_history(self):
        """Reset l·ªãch s·ª≠"""
        self.position_history.clear()
        print("[MLAgent] üîÑ History reset")
    
    def get_stats(self) -> dict:
        """Th·ªëng k√™"""
        return {
            "moves_evaluated": self.moves_evaluated,
            "history_length": len(self.position_history),
            "exploration_rate": self.exploration_rate
        }
